{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment 2 : Predictive Modelling of tip_amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "\n",
    "The goal is to develop a predictive model that estimates the tip amount given to taxi drivers. Performance must be evaluated using Mean Absolute Error (MAE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset\n",
    "Note that df_test represents the dataframe we have to use our model to predict tip_amount and X_test is a test set I defined for evaluating model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import base, pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from feature_engine import encoding, imputation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from typing import Any, Dict, Union\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "pd.options.mode.copy_on_write = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = 'test.csv'  \n",
    "df_test = pd.read_csv(file_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train.csv'  \n",
    "df_train = pd.read_csv(file_path)\n",
    "df_train = df_train.drop('weight', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline: Summary\n",
    "\n",
    "In this section, we've constructed data preprocessing pipeline that transforms, filters, and cleans the dataset. Breakdown: \n",
    "\n",
    "1. **Feature Engineering**: \n",
    "\n",
    "2. **Outlier Detection and Filtering**:\n",
    "3. **Additional Transformations**:\n",
    "   - Added weather information by merging external data.\n",
    "   - Replaced infinite and NaN values.\n",
    "   - Dropped irrelevant columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_time_hours(X): \n",
    "    X['length_time_hours'] = X['length_time'] / 3600\n",
    "    return X.drop('length_time', axis = 1)\n",
    "\n",
    "def trip_distance_km(X):\n",
    "    X['trip_distance_km'] = X['trip_distance'] * 1.60934\n",
    "    return X.drop('trip_distance', axis = 1)\n",
    "\n",
    "\n",
    "def calculate_velocity(X):\n",
    "    # Calculate velocity (trip_distance / length_time_hours) and assign it to a new column\n",
    "    X['velocity'] = np.divide(X['trip_distance_km'], X['length_time_hours'])\n",
    "    \n",
    "    return X\n",
    "\n",
    "def cost_per_mile(X): \n",
    "    # Calculate velocity (trip_distance / length_time_hours) and assign it to a new column\n",
    "    X['cost_per_km'] = np.divide(X['fare_amount'], X['trip_distance_km'])\n",
    "    \n",
    "    return X  \n",
    "\n",
    "def cost_per_hour(X): \n",
    "    # Calculate velocity (trip_distance / length_time_hours) and assign it to a new column\n",
    "    X['cost_per_hour'] = np.divide(X['fare_amount'], X['length_time_hours'] * 60)\n",
    "    \n",
    "    return X  \n",
    "\n",
    "def cost_per_pair(X):\n",
    "    \n",
    "    # Calculate mean fare_amount for each pair\n",
    "    pair_means = X.groupby('pair')['fare_amount'].mean()\n",
    "    \n",
    "    # Calculate fare_amount percentage variation from the mean of the pair group\n",
    "    X['fare_amount_pair_variation'] = X.apply(lambda row: ((row['fare_amount'] - pair_means[row['pair']]) / pair_means[row['pair']]) * 100, axis=1)\n",
    "    \n",
    "    return X\n",
    "    \n",
    "def fare_per_passenger(X): \n",
    "    # Compute passenger fare \n",
    "    X['fare_per_passenger'] = np.divide(X['fare_amount'], X['passenger_count'])\n",
    "\n",
    "    return X\n",
    "\n",
    "def estimated_fare(X): \n",
    "    def calculate_fare(row):\n",
    "        # Base fee and kilometer price\n",
    "        if row['pickup_hour'] >= 20 or (row['pickup_hour'] < 6 and row['pickup_wday'] in [5, 6]):\n",
    "            base_fee = 3.50  # Adjusted from $4.00 to $3.50\n",
    "        else:\n",
    "            base_fee = 2.50  # Adjusted from $3.00 to $2.50\n",
    "        kilometer_price = 1.75  # Adjusted from $2.18 to $1.75\n",
    "        \n",
    "        # Calculate distance-based fare\n",
    "        distance_fare = kilometer_price * row['trip_distance_km']\n",
    "        \n",
    "        # Waiting time fare\n",
    "        waiting_time_fare = 0.00  # Assume no waiting time information available\n",
    "        \n",
    "        # Apply surcharges\n",
    "        if row['pickup_hour'] >= 16 and row['pickup_hour'] < 20:\n",
    "            distance_fare += 1.00  # $1 surcharge for trips between 4pm to 8pm on weekdays\n",
    "        if row['dropoff_BoroCode'] in [1, 4, 5, 6, 9, 10, 12, 13]:  # Check if dropoff is within NYC\n",
    "            distance_fare += 0.50  # MTA State Surcharge\n",
    "        \n",
    "        # Total fare\n",
    "        total_fare = base_fee + distance_fare + waiting_time_fare\n",
    "        \n",
    "        return total_fare\n",
    "    \n",
    "    # Apply fare calculation function to each row and create a new column 'estimated_fare'\n",
    "    X['estimated_fare'] = X.apply(calculate_fare, axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def fare_differences(X): \n",
    "    X['diff_fares'] = X['fare_amount'] - X['estimated_fare']\n",
    "    return X\n",
    "\n",
    "def drop_columns(X): \n",
    "    X = copy.deepcopy(X)\n",
    "    #X.drop(['pickup_month', 'dropoff_BoroCode', 'pickup_BoroCode', \n",
    "    #           'pickup_NTACode', 'dropoff_NTACode', 'pickup_longitude', 'pickup_latitude', \n",
    "    #           'dropoff_longitude', 'dropoff_latitude', 'pickup_doy', 'pickup_week', 'velocity'], inplace = True, axis = 1)\n",
    "    \n",
    "    return X.drop(['pickup_month', 'dropoff_BoroCode', 'pickup_BoroCode', \n",
    "               'pickup_week', 'pickup_NTACode', 'dropoff_NTACode', 'estimated_fare', 'pair', 'pickup_datetime', \n",
    "               'pickup_dom', 'pickup_year', 'time', 'pickup_wday'], axis = 1)\n",
    "\n",
    "import calendar\n",
    "\n",
    "def add_datetime_column(X):\n",
    "    def is_leap_year(year):\n",
    "        return calendar.isleap(year)\n",
    "    \n",
    "    def day_of_year_to_day_of_month(year, day_of_year):\n",
    "        if is_leap_year(year):\n",
    "            days_in_month = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        else:\n",
    "            days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    \n",
    "        for month in range(12):\n",
    "            if sum(days_in_month[:month+1]) >= day_of_year:\n",
    "                return day_of_year - sum(days_in_month[:month])\n",
    "                \n",
    "    \n",
    "    # add day of month\n",
    "    X.loc[:, 'pickup_dom'] = X['pickup_doy'].apply(lambda x: day_of_year_to_day_of_month(2015, x))\n",
    "    # add year\n",
    "    X['pickup_year'] = 2015\n",
    "    # create the datetime of pick up column\n",
    "    X['pickup_datetime'] = pd.to_datetime({'year': X['pickup_year'],\n",
    "                                                   'month': X['pickup_month'],\n",
    "                                                   'day': X['pickup_dom'],\n",
    "                                                   'hour': X['pickup_hour']\n",
    "                                                  })\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def add_weather_info(df):\n",
    "    # open weather dataframe\n",
    "    df_weather = pd.read_csv('new_york_weather.csv', skiprows=2)\n",
    "    df_weather['time'] = pd.to_datetime(df_weather['time'])\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Store the original index\n",
    "    df_temp['original_index'] = df_temp.index\n",
    "    \n",
    "    # Sort required for merging\n",
    "    df_temp = df_temp.sort_values(by='pickup_datetime')\n",
    "    \n",
    "    # Merge dataframes on a temporary dataframe\n",
    "    df_temp = pd.merge_asof(df_temp, df_weather, left_on='pickup_datetime', right_on='time')\n",
    "    \n",
    "    # Join back to original indices to maintain order\n",
    "    df_return = pd.merge(df, df_temp, how='left', left_index=True, right_on='original_index')\n",
    "    df_return = df_return.set_index('original_index')  # Set original index back\n",
    "    \n",
    "    # Remove duplicate columns and rename columns\n",
    "    duplicate_columns = [col for col in df_return.columns if '_x' in col or '_y' in col]\n",
    "    for col in duplicate_columns:\n",
    "        new_col_name = col[:-2]  # Remove '_x' or '_y'\n",
    "        df_return.rename(columns={col: new_col_name}, inplace=True)\n",
    "    \n",
    "    # Remove exact duplicate columns\n",
    "    df_return = df_return.loc[:,~df_return.columns.duplicated()]\n",
    "    \n",
    "    return df_return\n",
    "\n",
    "def replace_inf_nan(X):\n",
    "    return X.replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_upperbound(X, feature):\n",
    "    feature_data = X[feature]\n",
    "    \n",
    "    Q1 = feature_data.quantile(0.25)\n",
    "    Q3 = feature_data.quantile(0.75)\n",
    "    \n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    return upper_bound\n",
    "\n",
    "def length_time_hours(X): \n",
    "    X['length_time_hours'] = X['length_time'] / 3600\n",
    "    return X.drop('length_time', axis = 1)\n",
    "\n",
    "def trip_distance_km(X):\n",
    "    X['trip_distance_km'] = X['trip_distance'] * 1.60934\n",
    "    return X.drop('trip_distance', axis = 1)\n",
    "\n",
    "def calculate_velocity(X):\n",
    "    X['velocity'] = np.divide(X['trip_distance_km'], X['length_time_hours'])\n",
    "    return X\n",
    "\n",
    "def filter_trip_distance_km(X): \n",
    "    upperbound_distance = calculate_upperbound(X, 'trip_distance_km')\n",
    "    upperbound_length_time = calculate_upperbound(X, 'length_time_hours')\n",
    "\n",
    "    # Case 1: Large distance, regular time\n",
    "    X_outliers = X[(X['trip_distance_km'] > upperbound_distance) & (X['length_time_hours'] <= upperbound_length_time)]\n",
    "    X_outliers = X_outliers[X_outliers['velocity'] > 100]\n",
    "    X.loc[X_outliers.index, 'trip_distance_km'] = np.NaN\n",
    "\n",
    "    # Case 2: Irregular distance, irregular time\n",
    "    X_outliers = X[(X['trip_distance_km'] > upperbound_distance) & (X['length_time_hours'] > upperbound_length_time)]\n",
    "    X_outliers = X_outliers[(X_outliers['velocity'] > 100) | (X_outliers['length_time_hours'] >= 3)]\n",
    "    X.loc[X_outliers.index, 'trip_distance_km'] = np.NaN\n",
    "\n",
    "    # Case 3: Zero cases\n",
    "    X_outliers = X[(X['length_time_hours'] == 0) | (X['trip_distance_km'] == 0)]\n",
    "    X.loc[X_outliers.index, 'trip_distance_km'] = np.NaN\n",
    "\n",
    "    # Case 4: Regular distance, regular time\n",
    "    X_outliers = X[(X['trip_distance_km'] <= upperbound_distance) & (X['length_time_hours'] <= upperbound_length_time)]\n",
    "    X_outliers = X_outliers[X_outliers['velocity'] > 100]\n",
    "    X.loc[X_outliers.index, 'trip_distance_km'] = np.NaN\n",
    "\n",
    "    return X\n",
    "\n",
    "def filter_length_time(X): \n",
    "    upperbound_distance = calculate_upperbound(X, 'trip_distance_km')\n",
    "    upperbound_length_time = calculate_upperbound(X, 'length_time_hours')\n",
    "\n",
    "    # Case 1: Regular distance, large time\n",
    "    X_outliers = X[(X['trip_distance_km'] <= upperbound_distance) & (X['length_time_hours'] > upperbound_length_time)]\n",
    "    X_outliers = X_outliers[(X_outliers['velocity'] > 100) | (X_outliers['length_time_hours'] >= 3)]\n",
    "    X.loc[X_outliers.index, 'length_time_hours'] = np.NaN\n",
    "\n",
    "    # Case 2: Zero cases\n",
    "    X_outliers = X[(X['length_time_hours'] == 0) | (X['trip_distance_km'] == 0)]\n",
    "    X.loc[X_outliers.index, 'length_time_hours'] = np.NaN\n",
    "\n",
    "    # Case 3: Irregular distance, irregular time\n",
    "    X_outliers = X[(X['trip_distance_km'] > upperbound_distance) & (X['length_time_hours'] > upperbound_length_time)]\n",
    "    X_outliers = X_outliers[(X_outliers['velocity'] > 100) | (X_outliers['length_time_hours'] >= 3)]\n",
    "    X.loc[X_outliers.index, 'length_time_hours'] = np.NaN\n",
    "\n",
    "    # Case 4: Regular distance, regular time\n",
    "    X_outliers = X[(X['trip_distance_km'] <= upperbound_distance) & (X['length_time_hours'] <= upperbound_length_time)]\n",
    "    X_outliers = X_outliers[X_outliers['velocity'] > 100]\n",
    "    X.loc[X_outliers.index, 'length_time_hours'] = np.NaN\n",
    "\n",
    "    return X\n",
    "\n",
    "def filter_fares(X):\n",
    "    upperbound_distance = calculate_upperbound(X, 'trip_distance_km')\n",
    "    upperbound_length_time = calculate_upperbound(X, 'length_time_hours')\n",
    "    upperbound_fare_amount = calculate_upperbound(X, 'fare_amount')\n",
    "\n",
    "    # Filter fares below 2.50\n",
    "    X_outliers = X[X['fare_amount'] < 2.50]\n",
    "    X.loc[X_outliers.index, 'fare_amount'] = np.NaN\n",
    "\n",
    "    # Filter irregular fare amounts\n",
    "    X_outliers = X[(X['trip_distance_km'] <= upperbound_distance) & \n",
    "                   (X['length_time_hours'] <= upperbound_length_time) & \n",
    "                   (X['fare_amount'] > upperbound_fare_amount)]\n",
    "    X.loc[X_outliers.index, 'fare_amount'] = np.NaN\n",
    "\n",
    "    return X\n",
    "\n",
    "def filter_passenger_count(X): \n",
    "    X.loc[X['passenger_count'] == 0, 'passenger_count'] = np.NaN\n",
    "    X.loc[X['passenger_count'] > 6, 'passenger_count'] = np.NaN\n",
    "\n",
    "    return X\n",
    "\n",
    "def convert_int_to_object(X):\n",
    "    X[['vendor_id', 'weather_code (wmo code)']] = X[['vendor_id', 'weather_code (wmo code)']].astype(object)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a series of feature-engineering, filtering, and transformation functions to the input DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing trip data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The transformed DataFrame with new features, filtered values, and selected columns.\n",
    "    \"\"\"\n",
    "    return (df\n",
    "            .pipe(length_time_hours)       # Convert length_time to hours and remove original column\n",
    "            .pipe(trip_distance_km)        # Convert trip_distance to kilometers and remove original column\n",
    "            .pipe(calculate_velocity)      # Calculate velocity using trip_distance_km and length_time_hours\n",
    "            .pipe(cost_per_mile)           # Calculate cost per kilometer\n",
    "            .pipe(cost_per_hour)           # Calculate cost per hour\n",
    "            .pipe(cost_per_pair)           # Calculate fare variation based on pair\n",
    "            .pipe(fare_per_passenger)      # Calculate fare per passenger\n",
    "            .pipe(estimated_fare)          # Estimate fare based on trip details\n",
    "            .pipe(fare_differences)        # Calculate the difference between actual and estimated fares\n",
    "            .pipe(add_datetime_column)     # Add datetime and year information\n",
    "            .pipe(add_weather_info)        # Add weather information by merging with external data\n",
    "            .pipe(replace_inf_nan)         # Replace infinite and NaN values\n",
    "            .pipe(filter_trip_distance_km) # Filter outliers based on trip distance and time\n",
    "            .pipe(filter_length_time)      # Filter outliers based on length of time and velocity\n",
    "            .pipe(filter_fares)            # Filter outliers based on fare amount and trip details\n",
    "            .pipe(filter_passenger_count)  # Filter invalid passenger counts\n",
    "            .pipe(convert_int_to_object)   # Convert categorical columns to type int\n",
    "            .pipe(drop_columns)            # Drop the specified columns\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweakDfTransformer(base.BaseEstimator, base.TransformerMixin): \n",
    "    def __init__(self, ycol=None): \n",
    "        self.ycol = ycol\n",
    "\n",
    "    def transform(self, X): \n",
    "        return tweak_df(X)\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the column names based on data type\n",
    "float_objects = [\n",
    "    'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'length_time_hours', 'trip_distance_km',\n",
    "    'velocity', 'cost_per_km', 'cost_per_hour', 'fare_amount_pair_variation', 'fare_per_passenger', 'diff_fares', 'temperature_2m (°C)', 'rain (mm)', \n",
    "    'passenger_count'\n",
    "]\n",
    "\n",
    "ordinal_encoding = ['pickup_hour', 'pickup_doy']\n",
    "\n",
    "# Assuming 'weather_code' and 'vendor_id' are the one-hot encoded variables\n",
    "one_hot_variables = ['vendor_id', 'weather_code (wmo code)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pl_pre = pipeline.Pipeline([\n",
    "    ('tweak', TweakDfTransformer()), \n",
    "    ('cat', encoding.OneHotEncoder(variables=one_hot_variables)),\n",
    "    ('float', imputation.MeanMedianImputer(imputation_method='mean', variables = float_objects))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying pre-processing pipelines \n",
    "Rather than tuning all of the parameters at once, we will use step-wise tuning. We will tune small groups of hyperparameters that act similarly, then move to the next group while preserving the values from the previous group. We're going to limit our steps to:\n",
    "\n",
    "- Tree parameters\n",
    "- Sampling parameters\n",
    "- Regularisation parameters\n",
    "- Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_train.drop(columns=[\"response\"]), df_train[\"response\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train_pre = df_pl_pre.fit_transform(X_train,y_train)\n",
    "X_test_pre = df_pl_pre.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_pre_scaled = pd.DataFrame(scaler.fit_transform(X_train_pre), columns = X_train_pre.columns)\n",
    "X_test_pre_scaled = pd.DataFrame(scaler.transform(X_test_pre), columns = X_test_pre.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-wise hyperarameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(\n",
    "    space: Dict[str, Union[float, int]], \n",
    "    X_train: pd.DataFrame, \n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame, \n",
    "    y_test: pd.Series,\n",
    "    early_stopping_rounds: int = 50,\n",
    "    n_estimators=1000,\n",
    "    metric: callable = mean_absolute_error ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for an XGBoost classifier.\n",
    "    \n",
    "    This function takes a dictionary of hyperparameters, training\n",
    "    and test data, and an optional value for early stopping rounds,\n",
    "    and returns a dictionary with the loss and model resulting from\n",
    "    the tuning process. The model is trained using the training\n",
    "    data and evaluated on the test data. The loss is computed as\n",
    "    the negative of the accuracy score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    space : Dict[str, Union[float, int]]\n",
    "        A dictionary of hyperparameters for the XGBoost classifier.\n",
    "    X_train : pd.DataFrame\n",
    "        The training data.\n",
    "    y_train : pd.Series\n",
    "        The training target.\n",
    "    X_test : pd.DataFrame\n",
    "        The test data.\n",
    "    y_test : pd.Series\n",
    "        The test target.\n",
    "    early_stopping_rounds : int, optional\n",
    "        The number of early stopping rounds to use. Default is 50.\n",
    "    metric : callable\n",
    "        Metric to maximize. Default is accuracy score.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        A dictionary with the loss and model resulting from the\n",
    "        tuning process. The loss is a float, and the model is an\n",
    "        XGBoost classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert integer parameters\n",
    "    int_vals = ['max_depth', 'reg_alpha']\n",
    "    space = {k: (int(val) if k in int_vals else val) for k, val in space.items()}\n",
    "    space['early_stopping_rounds'] = early_stopping_rounds\n",
    "    \n",
    "    # Instantiate the model with the given hyperparameters\n",
    "    model = xgb.XGBRegressor(**space, objective = \"reg:absoluteerror\")\n",
    "    \n",
    "    # Define evaluation sets\n",
    "    evaluation = [(X_train_pre_scaled, y_train), (X_test_pre_scaled, y_test)]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_pre_scaled, y_train,\n",
    "        eval_set=evaluation,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions and calculate the score\n",
    "    pred = model.predict(X_test_pre_scaled)\n",
    "    score = metric(y_test, pred)\n",
    "    \n",
    "    # Return the loss and model\n",
    "    return {'loss': score, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:03<00:00,  9.17s/trial, best loss: 0.5707543441464225]\n",
      "100%|██████████| 20/20 [02:43<00:00,  8.19s/trial, best loss: 0.5716850173111327]\n",
      "100%|██████████| 20/20 [03:08<00:00,  9.45s/trial, best loss: 0.5696087226332608]\n",
      "100%|██████████| 20/20 [03:20<00:00, 10.05s/trial, best loss: 0.5695489688957767]\n",
      "100%|██████████| 20/20 [02:47<00:00,  8.37s/trial, best loss: 0.5695556905128237]\n"
     ]
    }
   ],
   "source": [
    "rounds = [\n",
    "    {'max_depth': hp.quniform('max_depth', 1, 8, 1), 'min_child_weight': hp.loguniform('min_child_weight', -2, 3)},\n",
    "    {'subsample': hp.uniform('subsample', 0.5, 1), 'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)},\n",
    "    {'reg_alpha': hp.uniform('reg_alpha', 0, 10), 'reg_lambda': hp.uniform('reg_lambda', 1, 10)},\n",
    "    {'gamma': hp.loguniform('gamma', -10, 10)},\n",
    "    {'learning_rate': hp.loguniform('learning_rate', -7, 0)}\n",
    "]\n",
    "\n",
    "all_trials = []\n",
    "params = {}\n",
    "\n",
    "for round in rounds:\n",
    "    params.update(round)\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=lambda space: hyperparameter_tuning(space, X_train_pre_scaled, y_train, X_test_pre_scaled, y_test),\n",
    "        space=params,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,\n",
    "        trials=trials\n",
    "    )\n",
    "    params.update(best)\n",
    "    all_trials.append(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:1.04871\tvalidation_1-mae:1.05185\n",
      "[1]\tvalidation_0-mae:0.84132\tvalidation_1-mae:0.84301\n",
      "[2]\tvalidation_0-mae:0.72941\tvalidation_1-mae:0.73033\n",
      "[3]\tvalidation_0-mae:0.67255\tvalidation_1-mae:0.67313\n",
      "[4]\tvalidation_0-mae:0.63862\tvalidation_1-mae:0.63891\n",
      "[5]\tvalidation_0-mae:0.61709\tvalidation_1-mae:0.61737\n",
      "[6]\tvalidation_0-mae:0.60571\tvalidation_1-mae:0.60589\n",
      "[7]\tvalidation_0-mae:0.59835\tvalidation_1-mae:0.59851\n",
      "[8]\tvalidation_0-mae:0.59426\tvalidation_1-mae:0.59408\n",
      "[9]\tvalidation_0-mae:0.58961\tvalidation_1-mae:0.58956\n",
      "[10]\tvalidation_0-mae:0.58767\tvalidation_1-mae:0.58770\n",
      "[11]\tvalidation_0-mae:0.58671\tvalidation_1-mae:0.58676\n",
      "[12]\tvalidation_0-mae:0.58413\tvalidation_1-mae:0.58434\n",
      "[13]\tvalidation_0-mae:0.58292\tvalidation_1-mae:0.58332\n",
      "[14]\tvalidation_0-mae:0.58175\tvalidation_1-mae:0.58217\n",
      "[15]\tvalidation_0-mae:0.58131\tvalidation_1-mae:0.58172\n",
      "[16]\tvalidation_0-mae:0.58044\tvalidation_1-mae:0.58091\n",
      "[17]\tvalidation_0-mae:0.57948\tvalidation_1-mae:0.58006\n",
      "[18]\tvalidation_0-mae:0.57910\tvalidation_1-mae:0.57966\n",
      "[19]\tvalidation_0-mae:0.57896\tvalidation_1-mae:0.57957\n",
      "[20]\tvalidation_0-mae:0.57736\tvalidation_1-mae:0.57821\n",
      "[21]\tvalidation_0-mae:0.57661\tvalidation_1-mae:0.57763\n",
      "[22]\tvalidation_0-mae:0.57632\tvalidation_1-mae:0.57732\n",
      "[23]\tvalidation_0-mae:0.57600\tvalidation_1-mae:0.57699\n",
      "[24]\tvalidation_0-mae:0.57580\tvalidation_1-mae:0.57680\n",
      "[25]\tvalidation_0-mae:0.57493\tvalidation_1-mae:0.57599\n",
      "[26]\tvalidation_0-mae:0.57463\tvalidation_1-mae:0.57568\n",
      "[27]\tvalidation_0-mae:0.57444\tvalidation_1-mae:0.57549\n",
      "[28]\tvalidation_0-mae:0.57353\tvalidation_1-mae:0.57473\n",
      "[29]\tvalidation_0-mae:0.57328\tvalidation_1-mae:0.57448\n",
      "[30]\tvalidation_0-mae:0.57292\tvalidation_1-mae:0.57417\n",
      "[31]\tvalidation_0-mae:0.57271\tvalidation_1-mae:0.57403\n",
      "[32]\tvalidation_0-mae:0.57250\tvalidation_1-mae:0.57389\n",
      "[33]\tvalidation_0-mae:0.57234\tvalidation_1-mae:0.57379\n",
      "[34]\tvalidation_0-mae:0.57201\tvalidation_1-mae:0.57349\n",
      "[35]\tvalidation_0-mae:0.57193\tvalidation_1-mae:0.57343\n",
      "[36]\tvalidation_0-mae:0.57187\tvalidation_1-mae:0.57336\n",
      "[37]\tvalidation_0-mae:0.57126\tvalidation_1-mae:0.57258\n",
      "[38]\tvalidation_0-mae:0.57104\tvalidation_1-mae:0.57248\n",
      "[39]\tvalidation_0-mae:0.57077\tvalidation_1-mae:0.57225\n",
      "[40]\tvalidation_0-mae:0.57039\tvalidation_1-mae:0.57195\n",
      "[41]\tvalidation_0-mae:0.57009\tvalidation_1-mae:0.57176\n",
      "[42]\tvalidation_0-mae:0.56999\tvalidation_1-mae:0.57167\n",
      "[43]\tvalidation_0-mae:0.56977\tvalidation_1-mae:0.57146\n",
      "[44]\tvalidation_0-mae:0.56951\tvalidation_1-mae:0.57126\n",
      "[45]\tvalidation_0-mae:0.56935\tvalidation_1-mae:0.57114\n",
      "[46]\tvalidation_0-mae:0.56927\tvalidation_1-mae:0.57106\n",
      "[47]\tvalidation_0-mae:0.56917\tvalidation_1-mae:0.57097\n",
      "[48]\tvalidation_0-mae:0.56888\tvalidation_1-mae:0.57078\n",
      "[49]\tvalidation_0-mae:0.56877\tvalidation_1-mae:0.57071\n",
      "[50]\tvalidation_0-mae:0.56841\tvalidation_1-mae:0.57038\n",
      "[51]\tvalidation_0-mae:0.56833\tvalidation_1-mae:0.57033\n",
      "[52]\tvalidation_0-mae:0.56819\tvalidation_1-mae:0.57023\n",
      "[53]\tvalidation_0-mae:0.56811\tvalidation_1-mae:0.57019\n",
      "[54]\tvalidation_0-mae:0.56795\tvalidation_1-mae:0.57012\n",
      "[55]\tvalidation_0-mae:0.56793\tvalidation_1-mae:0.57010\n",
      "[56]\tvalidation_0-mae:0.56785\tvalidation_1-mae:0.57007\n",
      "[57]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[58]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[59]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[60]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[61]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[62]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[63]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[64]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[65]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[66]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[67]\tvalidation_0-mae:0.56775\tvalidation_1-mae:0.57000\n",
      "[68]\tvalidation_0-mae:0.56767\tvalidation_1-mae:0.56994\n",
      "[69]\tvalidation_0-mae:0.56767\tvalidation_1-mae:0.56994\n",
      "[70]\tvalidation_0-mae:0.56767\tvalidation_1-mae:0.56994\n",
      "[71]\tvalidation_0-mae:0.56767\tvalidation_1-mae:0.56994\n",
      "[72]\tvalidation_0-mae:0.56743\tvalidation_1-mae:0.56981\n",
      "[73]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[74]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[75]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[76]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[77]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[78]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[79]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[80]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[81]\tvalidation_0-mae:0.56727\tvalidation_1-mae:0.56962\n",
      "[82]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[83]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[84]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[85]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[86]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[87]\tvalidation_0-mae:0.56718\tvalidation_1-mae:0.56960\n",
      "[88]\tvalidation_0-mae:0.56713\tvalidation_1-mae:0.56955\n",
      "[89]\tvalidation_0-mae:0.56713\tvalidation_1-mae:0.56955\n",
      "[90]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[91]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[92]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[93]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[94]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[95]\tvalidation_0-mae:0.56703\tvalidation_1-mae:0.56946\n",
      "[96]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[97]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[98]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[99]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[100]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[101]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[102]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[103]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[104]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[105]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[106]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[107]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[108]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[109]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[110]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[111]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[112]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[113]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[114]\tvalidation_0-mae:0.56694\tvalidation_1-mae:0.56937\n",
      "[115]\tvalidation_0-mae:0.56690\tvalidation_1-mae:0.56939\n",
      "[116]\tvalidation_0-mae:0.56690\tvalidation_1-mae:0.56939\n",
      "[117]\tvalidation_0-mae:0.56690\tvalidation_1-mae:0.56939\n",
      "[118]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[119]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[120]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[121]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[122]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[123]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[124]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[125]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[126]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[127]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[128]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[129]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[130]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[131]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[132]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[133]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[134]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[135]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[136]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[137]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[138]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[139]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[140]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[141]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[142]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[143]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[144]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[145]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[146]\tvalidation_0-mae:0.56688\tvalidation_1-mae:0.56938\n",
      "[147]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[148]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[149]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[150]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[151]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[152]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[153]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[154]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[155]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[156]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[157]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[158]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[159]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[160]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[161]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[162]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[163]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[164]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[165]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[166]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[167]\tvalidation_0-mae:0.56645\tvalidation_1-mae:0.56902\n",
      "[168]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[169]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[170]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[171]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[172]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[173]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[174]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[175]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[176]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[177]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[178]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[179]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[180]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[181]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[182]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[183]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[184]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[185]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[186]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[187]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[188]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[189]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[190]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[191]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[192]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[193]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[194]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[195]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[196]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[197]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[198]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[199]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[200]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[201]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[202]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[203]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[204]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[205]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[206]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[207]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[208]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[209]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[210]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[211]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[212]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[213]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[214]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[215]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[216]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[217]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[218]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[219]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[220]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[221]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[222]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[223]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[224]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[225]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[226]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[227]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[228]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[229]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[230]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[231]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[232]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[233]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[234]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[235]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[236]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[237]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[238]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[239]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[240]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[241]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[242]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[243]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[244]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[245]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "[246]\tvalidation_0-mae:0.56632\tvalidation_1-mae:0.56892\n",
      "Test MAE: 0.5689157047349872\n"
     ]
    }
   ],
   "source": [
    "final_model = xgb.XGBRegressor(\n",
    "    max_depth=int(params['max_depth']),\n",
    "    min_child_weight=params['min_child_weight'],\n",
    "    subsample=params['subsample'],\n",
    "    colsample_bytree=params['colsample_bytree'],\n",
    "    reg_alpha=params['reg_alpha'],\n",
    "    reg_lambda=params['reg_lambda'],\n",
    "    gamma=params['gamma'],\n",
    "    learning_rate=params['learning_rate'],\n",
    "    early_stopping_rounds=50,\n",
    "    n_estimators=1000,\n",
    "    objective = \"reg:absoluteerror\"\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_pre_scaled, y_train, \n",
    "               eval_set = [(X_train_pre_scaled, y_train), (X_test_pre_scaled, y_test)])\n",
    "y_pred = final_model.predict(X_test_pre_scaled)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test MAE:\", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
